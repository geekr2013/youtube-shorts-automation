import os
import json
import requests
import time
from playwright.sync_api import sync_playwright
from urllib.parse import urljoin


class AAGAGCollector:
    """AAGAG ì‚¬ì´íŠ¸ì—ì„œ ë¹„ë””ì˜¤ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, download_dir="downloads", history_file="data/download_history.json"):
        self.download_dir = download_dir
        self.history_file = history_file
        self.base_url = "https://aagag.com"
        self.downloaded_ids = self.load_history()
        
        # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.download_dir, exist_ok=True)
        os.makedirs(os.path.dirname(self.history_file), exist_ok=True)
    
    def load_history(self):
        """ë‹¤ìš´ë¡œë“œ íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()
    
    def save_history(self):
        """ë‹¤ìš´ë¡œë“œ íˆìŠ¤í† ë¦¬ ì €ì¥"""
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.downloaded_ids), f, ensure_ascii=False, indent=2)
    
    def collect_posts(self, max_posts=50):
        """
        AAGAG ë©”ì¸ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ ë§í¬ ìˆ˜ì§‘
        
        Args:
            max_posts: ìˆ˜ì§‘í•  ìµœëŒ€ ê²Œì‹œë¬¼ ìˆ˜
            
        Returns:
            list: ê²Œì‹œë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"url": "...", "title": "..."}]
        """
        print(f"ğŸ“¡ AAGAG ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘...")
        posts = []
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
                page.goto(self.base_url, timeout=30000)
                page.wait_for_load_state("networkidle", timeout=30000)
                
                # ê²Œì‹œë¬¼ ë§í¬ ìˆ˜ì§‘ (a.article ë˜ëŠ” a.article.t ì…€ë ‰í„°)
                article_links = page.locator("a.article, a.article.t").all()
                
                print(f"âœ… ë°œê²¬í•œ ê²Œì‹œë¬¼ ë§í¬: {len(article_links)}ê°œ")
                
                for link in article_links[:max_posts]:
                    try:
                        href = link.get_attribute("href")
                        title = link.inner_text().strip()
                        
                        if href and title:
                            full_url = urljoin(self.base_url, href)
                            
                            # .mp4 íŒŒì¼ë§Œ í•„í„°ë§
                            if title.lower().endswith('.mp4'):
                                posts.append({
                                    "url": full_url,
                                    "title": title
                                })
                                print(f"  ğŸ¬ {title[:50]}... ({full_url})")
                    except Exception as e:
                        print(f"  âš ï¸ ê²Œì‹œë¬¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                
                print(f"âœ… .mp4 ë¹„ë””ì˜¤ ê²Œì‹œë¬¼ {len(posts)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            finally:
                browser.close()
        
        return posts
    
    def get_video_download_url(self, post_url):
        """
        ê°œë³„ ê²Œì‹œë¬¼ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ
        
        Args:
            post_url: ê²Œì‹œë¬¼ í˜ì´ì§€ URL
            
        Returns:
            str: ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ URL (ì—†ìœ¼ë©´ None)
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                # ê²Œì‹œë¬¼ í˜ì´ì§€ ë°©ë¬¸
                page.goto(post_url, timeout=30000)
                page.wait_for_load_state("networkidle", timeout=10000)
                
                # ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸° (https://i.aagag.com/*.mp4 íŒ¨í„´)
                # ë°©ë²• 1: a íƒœê·¸ì—ì„œ .mp4 ë§í¬ ì°¾ê¸°
                download_links = page.locator("a[href*='i.aagag.com'][href$='.mp4']").all()
                
                if download_links:
                    video_url = download_links[0].get_attribute("href")
                    print(f"    âœ… ë¹„ë””ì˜¤ URL ë°œê²¬: {video_url}")
                    browser.close()
                    return video_url
                
                # ë°©ë²• 2: í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ i.aagag.com ë§í¬ ì°¾ê¸°
                content = page.content()
                if "i.aagag.com" in content and ".mp4" in content:
                    import re
                    pattern = r'https://i\.aagag\.com/[A-Za-z0-9]+\.mp4'
                    matches = re.findall(pattern, content)
                    if matches:
                        video_url = matches[0]
                        print(f"    âœ… ë¹„ë””ì˜¤ URL ë°œê²¬ (ì •ê·œì‹): {video_url}")
                        browser.close()
                        return video_url
                
                print(f"    âš ï¸ ë¹„ë””ì˜¤ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
            except Exception as e:
                print(f"    âŒ ê²Œì‹œë¬¼ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜: {e}")
            finally:
                browser.close()
        
        return None
    
    def download_video(self, video_url, title):
        """
        ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        
        Args:
            video_url: ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ URL
            title: ë¹„ë””ì˜¤ ì œëª©
            
        Returns:
            str: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ None)
        """
        # íŒŒì¼ëª… ì •ë¦¬ (í™•ì¥ì ì œê±° í›„ .mp4 ì¶”ê°€)
        clean_title = title.replace('.mp4', '').replace('.MP4', '')
        # íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean_title = "".join(c for c in clean_title if c.isalnum() or c in (' ', '-', '_', '(', ')', '[', ']'))
        clean_title = clean_title.strip()[:100]  # ìµœëŒ€ 100ì
        
        filename = f"{clean_title}.mp4"
        filepath = os.path.join(self.download_dir, filename)
        
        try:
            print(f"    â¬‡ï¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {filename}")
            response = requests.get(video_url, stream=True, timeout=60)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
            print(f"    âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename} ({file_size:.2f} MB)")
            return filepath
            
        except Exception as e:
            print(f"    âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            if os.path.exists(filepath):
                os.remove(filepath)
            return None
    
    def collect_and_download(self, max_videos=5):
        """
        ê²Œì‹œë¬¼ ìˆ˜ì§‘ ë° ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        
        Args:
            max_videos: ë‹¤ìš´ë¡œë“œí•  ìµœëŒ€ ë¹„ë””ì˜¤ ìˆ˜
            
        Returns:
            list: ë‹¤ìš´ë¡œë“œëœ ë¹„ë””ì˜¤ ì •ë³´ [{"path": "...", "title": "..."}]
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ AAGAG ë¹„ë””ì˜¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_videos}ê°œ)")
        print(f"{'='*60}\n")
        
        # 1. ê²Œì‹œë¬¼ ìˆ˜ì§‘
        posts = self.collect_posts(max_posts=max_videos * 3)  # ì—¬ìœ ìˆê²Œ ìˆ˜ì§‘
        
        if not posts:
            print("âš ï¸ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 2. ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        downloaded_videos = []
        
        for i, post in enumerate(posts):
            if len(downloaded_videos) >= max_videos:
                print(f"\nâœ… ëª©í‘œ ê°œìˆ˜({max_videos}ê°œ) ë‹¬ì„±, ìˆ˜ì§‘ ì¢…ë£Œ")
                break
            
            post_url = post["url"]
            title = post["title"]
            
            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œí•œ ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸
            post_id = post_url.split("idx=")[-1] if "idx=" in post_url else post_url
            if post_id in self.downloaded_ids:
                print(f"\n[{i+1}/{len(posts)}] â­ï¸ ì´ë¯¸ ë‹¤ìš´ë¡œë“œí•œ ê²Œì‹œë¬¼: {title[:50]}...")
                continue
            
            print(f"\n[{i+1}/{len(posts)}] ğŸ¬ ì²˜ë¦¬ ì¤‘: {title[:50]}...")
            
            # 3. ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ
            video_url = self.get_video_download_url(post_url)
            
            if not video_url:
                print(f"    â­ï¸ ê±´ë„ˆë›°ê¸° (ë¹„ë””ì˜¤ URL ì—†ìŒ)")
                continue
            
            # 4. ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
            filepath = self.download_video(video_url, title)
            
            if filepath:
                downloaded_videos.append({
                    "path": filepath,
                    "title": title.replace('.mp4', '').replace('.MP4', '')
                })
                
                # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                self.downloaded_ids.add(post_id)
                self.save_history()
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
            time.sleep(2)
        
        print(f"\n{'='*60}")
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(downloaded_videos)}ê°œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ")
        print(f"{'='*60}\n")
        
        return downloaded_videos


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    collector = AAGAGCollector()
    videos = collector.collect_and_download(max_videos=3)
    
    print("\nğŸ“‹ ë‹¤ìš´ë¡œë“œëœ ë¹„ë””ì˜¤:")
    for video in videos:
        print(f"  - {video['title']}")
        print(f"    ê²½ë¡œ: {video['path']}")
