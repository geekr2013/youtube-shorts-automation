import re
import os
import time
import json
import requests
from datetime import datetime
from playwright.sync_api import sync_playwright

class AagagCollector:
    def __init__(self):
        self.base_url = 'https://aagag.com/issue/'
        self.history_file = 'data/download_history.json'
        self.downloaded_ids = self._load_history()
    
    def _load_history(self):
        """ë‹¤ìš´ë¡œë“œ ì´ë ¥ ë¡œë“œ"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            return set()
        except:
            return set()
    
    def _save_history(self):
        """ë‹¤ìš´ë¡œë“œ ì´ë ¥ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(self.history_file), exist_ok=True)
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.downloaded_ids), f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def get_video_posts(self, limit=20):
        """ë¹„ë””ì˜¤ ê²Œì‹œë¬¼ ëª©ë¡ ìˆ˜ì§‘"""
        print(f"ğŸ” AAGAG ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì¤‘...")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                page.goto(self.base_url, wait_until='networkidle', timeout=30000)
                time.sleep(2)
                
                # ê²Œì‹œë¬¼ ë§í¬ ì¶”ì¶œ
                posts = []
                links = page.query_selector_all('a.article')
                
                for link in links[:limit]:
                    try:
                        href = link.get_attribute('href')
                        text = link.inner_text()
                        
                        # idx ì¶”ì¶œ
                        match = re.search(r'idx=(\d+)', href)
                        if not match:
                            continue
                        
                        idx = match.group(1)
                        
                        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œí•œ ê²Œì‹œë¬¼ ê±´ë„ˆë›°ê¸°
                        if idx in self.downloaded_ids:
                            continue
                        
                        # ì œëª©ê³¼ ë©”íƒ€ë°ì´í„° íŒŒì‹±
                        lines = text.strip().split('\n')
                        title = lines[0] if lines else ''
                        
                        # ë¹„ë””ì˜¤ ê²Œì‹œë¬¼ í•„í„°ë§ (.gif, .mp4 í¬í•¨ ë˜ëŠ” íŒŒì¼ í¬ê¸°ê°€ í° ê²½ìš°)
                        is_video = any(ext in title.lower() for ext in ['.gif', '.mp4', '.webm', '.mov'])
                        
                        # íŒŒì¼ í¬ê¸° ì²´í¬ (ë³´í†µ ì˜ìƒì€ 0.5MB ì´ìƒ)
                        if not is_video:
                            size_match = re.search(r'([\d.]+)\s*MB', text)
                            if size_match:
                                size_mb = float(size_match.group(1))
                                is_video = size_mb >= 0.5
                        
                        if is_video:
                            posts.append({
                                'idx': idx,
                                'title': title,
                                'url': f"{self.base_url}?idx={idx}",
                                'raw_text': text
                            })
                    
                    except Exception as e:
                        print(f"âš ï¸ ê²Œì‹œë¬¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                        continue
                
                browser.close()
                print(f"âœ… ì´ {len(posts)}ê°œ ë¹„ë””ì˜¤ ê²Œì‹œë¬¼ ë°œê²¬")
                return posts
            
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                browser.close()
                return []
    
    def extract_media_url(self, post_url):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ë¯¸ë””ì–´ URL ì¶”ì¶œ"""
        print(f"ğŸ” ë¯¸ë””ì–´ URL ì¶”ì¶œ ì¤‘: {post_url}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                page.goto(post_url, wait_until='networkidle', timeout=30000)
                time.sleep(2)
                
                # video íƒœê·¸ í™•ì¸
                video_element = page.query_selector('video source')
                if video_element:
                    media_url = video_element.get_attribute('src')
                    if media_url:
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if media_url.startswith('//'):
                            media_url = 'https:' + media_url
                        elif media_url.startswith('/'):
                            media_url = 'https://aagag.com' + media_url
                        
                        browser.close()
                        print(f"âœ… ë¹„ë””ì˜¤ URL ë°œê²¬: {media_url}")
                        return media_url
                
                # img íƒœê·¸ í™•ì¸ (GIF)
                img_element = page.query_selector('img[src*=".gif"], img[src*=".mp4"]')
                if img_element:
                    media_url = img_element.get_attribute('src')
                    if media_url:
                        if media_url.startswith('//'):
                            media_url = 'https:' + media_url
                        elif media_url.startswith('/'):
                            media_url = 'https://aagag.com' + media_url
                        
                        browser.close()
                        print(f"âœ… ì´ë¯¸ì§€ URL ë°œê²¬: {media_url}")
                        return media_url
                
                browser.close()
                print(f"âš ï¸ ë¯¸ë””ì–´ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
            
            except Exception as e:
                print(f"âŒ ë¯¸ë””ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                browser.close()
                return None
    
    def download_video(self, media_url, idx, output_dir='data/videos'):
        """ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # íŒŒì¼ í™•ì¥ì ê²°ì •
            ext = '.mp4'
            if '.gif' in media_url.lower():
                ext = '.gif'
            elif '.webm' in media_url.lower():
                ext = '.webm'
            
            output_path = os.path.join(output_dir, f'aagag_{idx}{ext}')
            
            print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {media_url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Referer': 'https://aagag.com/'
            }
            
            response = requests.get(media_url, headers=headers, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # ë‹¤ìš´ë¡œë“œ ì´ë ¥ì— ì¶”ê°€
            self.downloaded_ids.add(idx)
            self._save_history()
            
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path}")
            return output_path
        
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return None
