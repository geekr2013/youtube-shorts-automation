"""
AAGAG ì½˜í…ì¸  ìˆ˜ì§‘ê¸° - ê°¤ëŸ¬ë¦¬ í˜ì´ì§€ ì§€ì› + í•œê¸€ ì¸ì½”ë”© ê°•í™” ë²„ì „
ì—¬ëŸ¬ ê°œì˜ ë¹„ë””ì˜¤/GIFê°€ ìˆëŠ” ê²½ìš° ëª¨ë‘ ìˆ˜ì§‘
"""

import os
import re
import time
import unicodedata
from pathlib import Path
from typing import List, Dict, Optional
import logging
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AAGAGCollector:
    """AAGAG ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ - ê°¤ëŸ¬ë¦¬ ì§€ì› + í•œê¸€ ì•ˆì „ ì²˜ë¦¬"""
    
    def __init__(self, base_url: str = "https://aagag.com/issue/"):
        self.base_url = base_url
        self.download_dir = Path("data/videos")
        self.download_dir.mkdir(parents=True, exist_ok=True)
        
        # ë‹¤ìš´ë¡œë“œ ì´ë ¥ ê´€ë¦¬
        self.history_file = Path("data/download_history.json")
        self.downloaded_urls = self._load_history()
    
    def _load_history(self) -> set:
        """ë‹¤ìš´ë¡œë“œ ì´ë ¥ ë¡œë“œ"""
        if self.history_file.exists():
            import json
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()
    
    def _save_history(self):
        """ë‹¤ìš´ë¡œë“œ ì´ë ¥ ì €ì¥"""
        import json
        self.history_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.downloaded_urls), f, indent=2, ensure_ascii=False)
    
    def _normalize_korean_text(self, text: str) -> str:
        """
        í•œê¸€ í…ìŠ¤íŠ¸ ì •ê·œí™” (NFC ì •ê·œí™” + ì•ˆì „ ì²˜ë¦¬)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""
        
        # NFC ì •ê·œí™” (í•œê¸€ ì¡°í•©í˜• í†µì¼)
        normalized = unicodedata.normalize('NFC', text)
        
        # ì œì–´ ë¬¸ì ì œê±°
        normalized = ''.join(char for char in normalized if unicodedata.category(char)[0] != 'C')
        
        return normalized.strip()
    
    def _safe_filename(self, text: str, max_length: int = 50) -> str:
        """
        ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„± (í•œê¸€ ë³´ì¡´)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ê¸¸ì´
            
        Returns:
            ì•ˆì „í•œ íŒŒì¼ëª…
        """
        # 1. í•œê¸€ ì •ê·œí™”
        text = self._normalize_korean_text(text)
        
        # 2. íŒŒì¼ ì‹œìŠ¤í…œ ê¸ˆì§€ ë¬¸ì ì œê±° (Windows/Linux í˜¸í™˜)
        forbidden_chars = r'[<>:"/\\|?*\x00-\x1f]'
        safe_text = re.sub(forbidden_chars, '', text)
        
        # 3. ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ
        safe_text = re.sub(r'\s+', ' ', safe_text)
        
        # 4. ì•ë’¤ ê³µë°± ë° ì  ì œê±°
        safe_text = safe_text.strip('. ')
        
        # 5. ê¸¸ì´ ì œí•œ (ë°”ì´íŠ¸ ê¸°ì¤€ì´ ì•„ë‹Œ ë¬¸ì ê¸°ì¤€)
        if len(safe_text) > max_length:
            safe_text = safe_text[:max_length].strip()
        
        # 6. ë¹ˆ ë¬¸ìì—´ ë°©ì§€
        if not safe_text:
            safe_text = f"video_{int(time.time())}"
        
        return safe_text
    
    def collect_and_download(self, max_videos: int = 5) -> List[Dict]:
        """
        ê²Œì‹œë¬¼ ìˆ˜ì§‘ ë° ë‹¤ìš´ë¡œë“œ (ê°¤ëŸ¬ë¦¬ ì§€ì›)
        
        Args:
            max_videos: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            ë‹¤ìš´ë¡œë“œëœ ë¹„ë””ì˜¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("\n" + "="*60)
        logger.info(f"ğŸš€ AAGAG ë¹„ë””ì˜¤/GIF ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_videos}ê°œ)")
        logger.info("="*60 + "\n")
        
        collected_videos = []
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                # 1. ë©”ì¸ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ ë§í¬ ìˆ˜ì§‘
                logger.info("ğŸ“¡ AAGAG ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘...")
                page.goto(self.base_url, wait_until="domcontentloaded", timeout=30000)
                page.wait_for_timeout(2000)
                
                # ê²Œì‹œë¬¼ ë§í¬ ì¶”ì¶œ (ëª¨ë“  ê²Œì‹œë¬¼)
                post_links = page.eval_on_selector_all(
                    'a.article, a.article.t',
                    'elements => elements.map(e => e.href)'
                )
                
                logger.info(f"âœ… ë°œê²¬í•œ ê²Œì‹œë¬¼ ë§í¬: {len(post_links)}ê°œ\n")
                
                # 2. ê° ê²Œì‹œë¬¼ ë°©ë¬¸í•˜ì—¬ ëª¨ë“  ë‹¤ìš´ë¡œë“œ ë§í¬ í™•ì¸
                checked_count = 0
                for post_url in post_links:
                    if len(collected_videos) >= max_videos:
                        break
                    
                    checked_count += 1
                    logger.info(f"ğŸ” [{checked_count}/{len(post_links)}] ê²Œì‹œë¬¼ í™•ì¸ ì¤‘")
                    logger.info(f"   {post_url}")
                    
                    try:
                        # ê²Œì‹œë¬¼ ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸
                        page.goto(post_url, wait_until="domcontentloaded", timeout=15000)
                        page.wait_for_timeout(1500)
                        
                        # ëª¨ë“  ë¯¸ë””ì–´ URL ì¶”ì¶œ (ê°¤ëŸ¬ë¦¬ ì§€ì›)
                        media_urls = self._extract_all_media_urls(page)
                        
                        if media_urls:
                            logger.info(f"   ğŸ“¦ ë°œê²¬í•œ ë¯¸ë””ì–´: {len(media_urls)}ê°œ")
                            
                            # ê° ë¯¸ë””ì–´ ì²˜ë¦¬
                            for idx, media_url in enumerate(media_urls):
                                if len(collected_videos) >= max_videos:
                                    break
                                
                                # ì´ë¯¸ ë‹¤ìš´ë¡œë“œí•œ URL ìŠ¤í‚µ
                                if media_url in self.downloaded_urls:
                                    logger.debug(f"   â­ï¸ ì´ë¯¸ ë‹¤ìš´ë¡œë“œí•¨: {media_url}")
                                    continue
                                
                                file_ext = self._get_file_extension(media_url)
                                
                                if file_ext in ['.mp4', '.gif']:
                                    # ì œëª© ìƒì„± (ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ë²ˆí˜¸ ì¶”ê°€)
                                    base_title = self._extract_title(page, media_url)
                                    if len(media_urls) > 1:
                                        title = f"{base_title}_{idx+1}"
                                    else:
                                        title = base_title
                                    
                                    logger.info(f"   âœ… [{idx+1}/{len(media_urls)}] {title} ({file_ext})")
                                    
                                    # ë‹¤ìš´ë¡œë“œ
                                    video_path = self._download_file(media_url, title, file_ext)
                                    
                                    if video_path:
                                        # GIF â†’ MP4 ë³€í™˜
                                        if file_ext == '.gif':
                                            video_path = self._convert_gif_to_mp4(video_path)
                                        
                                        collected_videos.append({
                                            'title': title,
                                            'video_path': str(video_path),
                                            'source_url': post_url,
                                            'download_url': media_url,
                                            'type': file_ext
                                        })
                                        
                                        # ì´ë ¥ì— ì¶”ê°€
                                        self.downloaded_urls.add(media_url)
                                        self._save_history()
                            
                            logger.info("")  # ë¹ˆ ì¤„
                        else:
                            logger.debug(f"   â­ï¸ ìŠ¤í‚µ (ë¯¸ë””ì–´ ì—†ìŒ)\n")
                    
                    except Exception as e:
                        logger.warning(f"   âš ï¸ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\n")
                        continue
                
                logger.info(f"âœ… ë¹„ë””ì˜¤/GIF ê²Œì‹œë¬¼ {len(collected_videos)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\n")
                
            except Exception as e:
                logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}\n")
            
            finally:
                browser.close()
        
        return collected_videos
    
    def _extract_all_media_urls(self, page) -> List[str]:
        """
        ê²Œì‹œë¬¼ í˜ì´ì§€ì—ì„œ ëª¨ë“  ë¯¸ë””ì–´ URL ì¶”ì¶œ (ê°¤ëŸ¬ë¦¬ ì§€ì›)
        
        Returns:
            ë¯¸ë””ì–´ URL ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°ë¨)
        """
        media_urls = []
        
        try:
            # ë°©ë²• 1: <img> íƒœê·¸ì—ì„œ i.aagag.com ì´ë¯¸ì§€/GIF ì°¾ê¸°
            img_sources = page.eval_on_selector_all(
                'img',
                '''elements => elements
                    .map(e => e.src)
                    .filter(src => src && src.includes('i.aagag.com'))
                '''
            )
            
            for src in img_sources:
                # ì¸ë„¤ì¼/ë¯¸ë‹ˆ ì´ë¯¸ì§€ ì œì™¸, ì‹¤ì œ íŒŒì¼ë§Œ
                if '/mini/' not in src and '/200x170/' not in src:
                    # .jpgë¥¼ .gifë‚˜ .mp4ë¡œ ë³€í™˜ ì‹œë„
                    if src.endswith('.jpg'):
                        # ê°™ì€ íŒŒì¼ëª…ì˜ .gifì™€ .mp4 ì‹œë„
                        base_url = src.rsplit('.', 1)[0]
                        media_urls.append(f"{base_url}.gif")
                        media_urls.append(f"{base_url}.mp4")
                    else:
                        media_urls.append(src)
            
            # ë°©ë²• 2: í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ì§ì ‘ i.aagag.com ë§í¬ ì°¾ê¸°
            content = page.content()
            
            # MP4 íŒ¨í„´
            mp4_pattern = r'https://i\.aagag\.com/[A-Za-z0-9]+\.mp4'
            mp4_matches = re.findall(mp4_pattern, content)
            media_urls.extend(mp4_matches)
            
            # GIF íŒ¨í„´
            gif_pattern = r'https://i\.aagag\.com/[A-Za-z0-9]+\.gif'
            gif_matches = re.findall(gif_pattern, content)
            media_urls.extend(gif_matches)
            
            # ë°©ë²• 3: <a> íƒœê·¸ì˜ href í™•ì¸
            links = page.eval_on_selector_all(
                'a',
                '''elements => elements
                    .map(e => e.href)
                    .filter(href => href && href.includes('i.aagag.com') && 
                           (href.endsWith('.mp4') || href.endsWith('.gif')))
                '''
            )
            media_urls.extend(links)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_urls = list(dict.fromkeys(media_urls))
            
            # ì‹¤ì œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (HEAD ìš”ì²­)
            valid_urls = []
            for url in unique_urls:
                if self._check_url_exists(url):
                    valid_urls.append(url)
            
            return valid_urls
        
        except Exception as e:
            logger.debug(f"ë¯¸ë””ì–´ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _check_url_exists(self, url: str) -> bool:
        """URLì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (HEAD ìš”ì²­)"""
        try:
            import requests
            response = requests.head(url, timeout=5, allow_redirects=True)
            return response.status_code == 200
        except:
            return False
    
    def _get_file_extension(self, url: str) -> str:
        """URLì—ì„œ íŒŒì¼ í™•ì¥ì ì¶”ì¶œ"""
        match = re.search(r'\.(mp4|gif|webm|avi)(?:\?|$)', url.lower())
        return f".{match.group(1)}" if match else ""
    
    def _extract_title(self, page, download_url: str) -> str:
        """ì œëª© ì¶”ì¶œ (í˜ì´ì§€ ì œëª© ë˜ëŠ” íŒŒì¼ëª…) - í•œê¸€ ì•ˆì „ ì²˜ë¦¬"""
        try:
            # ë°©ë²• 1: í˜ì´ì§€ ì œëª©
            title = page.title()
            if title and title != "AAGAG":
                # ë¶ˆí•„ìš”í•œ ì ‘ë¯¸ì‚¬ ì œê±°
                title = re.sub(r'\s*-\s*AAGAG.*$', '', title)
                # ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜
                return self._safe_filename(title, max_length=50)
            
            # ë°©ë²• 2: URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
            filename = download_url.split('/')[-1].split('?')[0]
            return re.sub(r'\.(mp4|gif)$', '', filename)
        
        except:
            return f"video_{int(time.time())}"
    
    def _download_file(self, url: str, title: str, ext: str) -> Optional[Path]:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ - í•œê¸€ ì•ˆì „ ì²˜ë¦¬"""
        try:
            import requests
            
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„± (í•œê¸€ ë³´ì¡´)
            safe_title = self._safe_filename(title)
            filename = f"{safe_title}{ext}"
            filepath = self.download_dir / filename
            
            # íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€
            counter = 1
            while filepath.exists():
                filename = f"{safe_title}_{counter}{ext}"
                filepath = self.download_dir / filename
                counter += 1
            
            logger.info(f"      ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {filename}")
            
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # UTF-8ë¡œ ì•ˆì „í•˜ê²Œ ì €ì¥
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            size_mb = len(response.content) / 1024 / 1024
            logger.info(f"      âœ… ì™„ë£Œ: {filepath.name} ({size_mb:.2f} MB)")
            return filepath
        
        except Exception as e:
            logger.error(f"      âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _convert_gif_to_mp4(self, gif_path: Path) -> Path:
        """GIFë¥¼ MP4ë¡œ ë³€í™˜ - ì›ë³¸ ìŒì„± ë³´ì¡´"""
        try:
            import subprocess
            
            mp4_path = gif_path.with_suffix('.mp4')
            
            logger.info(f"      ğŸ”„ GIF â†’ MP4 ë³€í™˜ ì¤‘...")
            
            # YouTube Shorts í˜¸í™˜ ì„¤ì •
            cmd = [
                'ffmpeg',
                '-i', str(gif_path),
                '-vf', 'scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2,setsar=1',
                '-c:v', 'libx264',
                '-pix_fmt', 'yuv420p',
                '-preset', 'medium',
                '-crf', '23',
                '-movflags', '+faststart',
                '-y',
                str(mp4_path)
            ]
            
            subprocess.run(cmd, check=True, capture_output=True)
            
            # ì›ë³¸ GIF ì‚­ì œ
            gif_path.unlink()
            
            logger.info(f"      âœ… ë³€í™˜ ì™„ë£Œ: {mp4_path.name}")
            return mp4_path
        
        except Exception as e:
            logger.error(f"      âŒ GIF ë³€í™˜ ì‹¤íŒ¨: {e}")
            return gif_path


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    collector = AAGAGCollector()
    videos = collector.collect_and_download(max_videos=3)
    
    print(f"\nìˆ˜ì§‘ëœ ë¹„ë””ì˜¤: {len(videos)}ê°œ")
    for video in videos:
        print(f"  - {video['title']}: {video['video_path']}")


if __name__ == "__main__":
    main()
